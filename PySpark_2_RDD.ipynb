{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Tutorial Part 2\n",
    "\n",
    "## RDD (Resilient Distributed Dataset)\n",
    "\n",
    "### Terminologies\n",
    "\n",
    "RDD stands for Resilient Distributed Dataset, these are the elements that run and operate on multiple nodes to do parallel processing on a cluster.\n",
    "\n",
    "RDDs are...\n",
    "* immutable\n",
    "* fault tolerant / automatic recovery\n",
    "* can apply multiple ops on RDDs\n",
    "\n",
    "RDD operation are...\n",
    "* Transformation\n",
    "* Action\n",
    "\n",
    "### Basic Operations (Ops)\n",
    "- `count()`: Number of elements in the RDD is returned.\n",
    "- `collect()`: All the elements in the RDD are returned.\n",
    "- `foreach(f)`: input callable, and returns only those elements which meet the condition of the function inside foreach.\n",
    "- `filter(f)`: input callable, and returns new RDDs containing the elements which satisfy the given callable\n",
    "- `map(f, preservesPartitioning = False)`: A new RDD is returned by applying a function to each element in the RDD\n",
    "- `reduce(f)`: After performing the specified commutative and associative binary operation, the element in the RDD is returned.\n",
    "- `join(other, numPartitions = None)`: It returns RDD with a pair of elements with the matching keys and all the values for that particular key. \n",
    "- `cache()`: Persist this RDD with the default storage level (MEMORY_ONLY). You can also check if the RDD is cached or not.\n",
    "\n",
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import modules\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of elements in RDD: 8\n"
     ]
    }
   ],
   "source": [
    "### count() ###\n",
    "\n",
    "sc = SparkContext('local', 'count app') # master, appName\n",
    "words = [\"scala\", \"java\", \"hadoop\", \"spark\", \"akka\",\n",
    "         \"spark vs hadoop\", \"pyspark\",\"pyspark and spark\"]\n",
    "\n",
    "rdd = sc.parallelize(words)\n",
    "count = rdd.count()\n",
    "print(f'Num of elements in RDD: {count}')\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elements in rdd, using collect(): \n",
      "['scala', 'java', 'hadoop', 'spark', 'akka', 'spark vs hadoop', 'pyspark', 'pyspark and spark']\n"
     ]
    }
   ],
   "source": [
    "### Collect ###\n",
    "\n",
    "sc = SparkContext('local', 'collect app')\n",
    "words = [\"scala\", \"java\", \"hadoop\", \"spark\", \"akka\",\n",
    "         \"spark vs hadoop\", \"pyspark\",\"pyspark and spark\"]\n",
    "\n",
    "rdd = sc.parallelize(words)\n",
    "coll = rdd.collect()\n",
    "print(f'elements in rdd, using collect(): \\n{coll}')\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### foreach(f)\n",
    "# here we pass in print function as arg\n",
    "\n",
    "sc = SparkContext('local', 'foreach app')\n",
    "words = [\"scala\", \"java\", \"hadoop\", \"spark\", \"akka\",\n",
    "         \"spark vs hadoop\", \"pyspark\",\"pyspark and spark\"]\n",
    "\n",
    "rdd = sc.parallelize(words)\n",
    "def f(x): print(f'print func: {x}')\n",
    "fore = rdd.foreach(f)\n",
    "sc.stop()\n",
    "## doesn't work??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered words, contains 'spark': \n",
      "['spark', 'spark vs hadoop', 'pyspark', 'pyspark and spark']\n"
     ]
    }
   ],
   "source": [
    "### Filter ###\n",
    "# filtering out the words that includes 'spark'\n",
    "sc.stop()\n",
    "sc = SparkContext('local', 'filter app')\n",
    "words = [\"scala\", \"java\", \"hadoop\", \"spark\", \"akka\",\n",
    "         \"spark vs hadoop\", \"pyspark\",\"pyspark and spark\"]\n",
    "\n",
    "rdd = sc.parallelize(words)\n",
    "filtered = rdd.filter(lambda arg: 'spark' in arg) # pythonRDD obj\n",
    "collected = filtered.collect()\n",
    "print(f'filtered words, contains \\'spark\\': \\n{collected}')\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapped words, map(): [[123, 107, 105, 116, 105], [114, 105, 126, 105], [112, 105, 108, 119, 119, 120], [123, 120, 105, 122, 115], [105, 115, 115, 105], [123, 120, 105, 122, 115, 40, 126, 123, 40, 112, 105, 108, 119, 119, 120], [120, 129, 123, 120, 105, 122, 115], [120, 129, 123, 120, 105, 122, 115, 40, 105, 118, 108, 40, 123, 120, 105, 122, 115]]\n"
     ]
    }
   ],
   "source": [
    "### map ###\n",
    "\n",
    "sc.stop()\n",
    "sc = SparkContext('local', 'map app')\n",
    "words = [\"scala\", \"java\", \"hadoop\", \"spark\", \"akka\",\n",
    "         \"spark vs hadoop\", \"pyspark\",\"pyspark and spark\"]\n",
    "\n",
    "rdd = sc.parallelize(words)\n",
    "\n",
    "def cipher(words):\n",
    "    '''\n",
    "    encrypt character by character using unicode, and shift\n",
    "    '''\n",
    "    return [ord(ch) + 8 for ch in words]\n",
    "\n",
    "mapped = rdd.map(cipher)\n",
    "ans = mapped.collect()\n",
    "print(f'mapped words, map(): {ans}')\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "addition using reduce. this returns eldements in RDD.\n",
      "Result: 15\n"
     ]
    }
   ],
   "source": [
    "### reduce() ###\n",
    "# apply add ops on the elements in list\n",
    "\n",
    "from operator import add\n",
    "sc.stop()\n",
    "sc = SparkContext('local', 'reduce app')\n",
    "nums = [1, 2, 3, 4, 5]\n",
    "rdd = sc.parallelize(nums)\n",
    "result = rdd.reduce(add)\n",
    "print(f'addition using reduce. this returns eldements in RDD.\\nResult: {result}')\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joined RDD: \n",
      "[('hadoop', (4, 2)), ('spark', (1, 7))]\n"
     ]
    }
   ],
   "source": [
    "### join() ###\n",
    "\n",
    "'''In the following example, there are two pair of elements\n",
    "in two different RDDs. After joining these two RDDs, \n",
    "we get an RDD with elements having matching keys and their values.'''\n",
    "\n",
    "sc.stop()\n",
    "sc = SparkContext('local', 'join app')\n",
    "x = sc.parallelize([(\"spark\", 1), (\"hadoop\", 4)])\n",
    "y = sc.parallelize([('spark', 7), ('hadoop', 2)])\n",
    "\n",
    "union = x.join(y)\n",
    "result = union.collect()\n",
    "print(f'joined RDD: \\n{result}')\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before cache: False\n",
      "after persist: True\n"
     ]
    }
   ],
   "source": [
    "### cache ###\n",
    "\n",
    "sc.stop()\n",
    "sc = SparkContext('local', 'cache app')\n",
    "words = [\"scala\", \"java\", \"hadoop\", \"spark\", \"akka\",\n",
    "         \"spark vs hadoop\", \"pyspark\",\"pyspark and spark\"]\n",
    "\n",
    "rdd = sc.parallelize(words)\n",
    "caching = rdd.is_cached\n",
    "print(f'before cache: {caching}')\n",
    "rdd.cache()\n",
    "caching = rdd.persist().is_cached\n",
    "print(f'after persist: {caching}')\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources, links\n",
    "\n",
    "- [How to stop a running SparkContext before opening the new one](https://stackoverflow.com/questions/36844075/how-to-stop-a-running-sparkcontext-before-opening-the-new-one)\n",
    "- [\\*args and \\*\\*kwargs in python explained](https://pythontips.com/2013/08/04/args-and-kwargs-in-python-explained/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
